{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart guide\n",
    "\n",
    "In this notebook we will through all the steps from downloading the data and training a model to evaluating the results. Check out the `environment.yml` file for the required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false"
   },
   "source": [
    "## Downloading the data\n",
    "\n",
    "The data is hosted here. For this guide we will simply download the 500 hPa geopotential data (Z500)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\programdata\\anaconda3\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# This might take a few minutes\n",
    "!wget \"https://dataserv.ub.tum.de/s/m1524895/download?path=%2F5.625deg%2Fgeopotential_500&files=geopotential_500_5.625deg.zip\" -O geopotential_500_5.625deg.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data\n",
    "import wget\n",
    "dataserv = wget.download(\"https://dataserv.ub.tum.de/s/m1524895/download?path=%2F5.625deg%2Fgeopotential_500&files=geopotential_500_5.625deg.zip\")\n",
    "# https://pypi.org/project/wget/\n",
    "# wget : get files from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_list = './geopotential_500'\n",
    "#os.mkdir(path_list)\n",
    "\n",
    "# !mkdir -p geopotential_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip\n",
    "import zipfile\n",
    "try:\n",
    "    with zipfile.ZipFile(\"geopotential_500_5.625deg.zip\") as zf:\n",
    "        zf.extractall('geopotential_500')\n",
    "        print(\"uncompress success\")\n",
    "except:\n",
    "    print(\"uncompress fail\")\n",
    "    \n",
    "# !unzip -d geopotential_500/ geopotential_500_5.625deg.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false"
   },
   "source": [
    "## Open the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the *.nc format data \n",
    "z500 = xr.open_mfdataset('geopotential_500/*.nc', combine='by_coords')\n",
    "#http://xarray.pydata.org/en/stable/generated/xarray.open_mfdataset.html\n",
    "# error for Korean file / folder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z500\n",
    "# Coordinates : labels of longtitude, latitude, time\n",
    "# Data Variables : z (z500) (whose each value has the labels of lon, lat, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an example\n",
    "z500.z.isel(time=0).plot();\n",
    "# xarray.variable.isel(label=index) # select by index \n",
    "\n",
    "# cf.\n",
    "# xarray[:, 0] # select by integer index \n",
    "# xarray.loc[:, 'IA'] # select by label name\n",
    "# xarray.sel(space='IA') # select by label name\n",
    "# xarray.isel(space=0) # select by integer index \n",
    "\n",
    "# xarray.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false"
   },
   "source": [
    "## Create a simple climatological forecast\n",
    "\n",
    "Remember that we are using the years 2017 and 2018 for testing/evaluation, so we are not allowed to use these years to train any data-driven model.\n",
    "\n",
    "For more information on the climatology and persistence forecasts used in the paper check out `notebooks/1-climatology-persistence.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed things up we will just take the mean for 2016\n",
    "climatology = z500.sel(time=slice('2016', '2016')).mean('time').load()\n",
    "# select by index 'time' from 2016-01-01 00:00:00 to 2016-12-31 23:00:00\n",
    "# calculate mean by whole time (in 2016) -> then, we get blurly image\n",
    "\n",
    "# .load() : load data into RAM. Unlike compute(), the original dataset is modified and returned.\n",
    "# If we don't call load(), the dataset just exist as a dask array(not in memory)\n",
    "# But, we can plot the dataset without calling load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "climatology.z.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false"
   },
   "source": [
    "### Evaluate the climatology\n",
    "\n",
    "Please check the paper for details on the evaluation metric. Here we will use the functions from `src/score.py`. To make sure we are always using the same targets for testing, we also implemented a function to load the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.score import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z500_test = load_test_data('geopotential_500/', 'z')[::12]  # Take data only every 12 hours to speed up computation on Binder\n",
    "\n",
    "# load_test_data(path, z or t) : load z (geopotential) data by slicing from 2017 to 2018 (the range of test dataset)\n",
    "\n",
    "# cf. \n",
    "# array[a:b:c] : start 'a' end 'b' every 'c' step\n",
    "# array[::c] : end to end with every 'c' step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_climatology = compute_weighted_rmse(climatology.z, z500_test).load()\n",
    "\n",
    "# compute_weighted_rmse( forcast object: z, test target: z500_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 왜 weight by latitude를 쓸까?\n",
    "# weight를 쓰면 값에 차이가 있나? \n",
    "# 왜 본래 데이터에 weight를 하지 않고 rmse를 할 때 weight를 곱하나?\n",
    "\n",
    "### RMSE without cosine weighting\n",
    "da_fc = climatology.z.load()\n",
    "da_true = z500_test.load()\n",
    "error = da_fc - da_true\n",
    "\n",
    "rmse1 = np.sqrt(((error)**2).mean()) \n",
    "rmse1 # 1269.5848\n",
    "\n",
    "### RMSE with cosine weighting\n",
    "da_fc = climatology.z.load()\n",
    "da_true = z500_test.load()\n",
    "error = da_fc - da_true\n",
    "\n",
    "weights_lat = np.cos(np.deg2rad(error.lat))\n",
    "weights_lat /= weights_lat.mean() # a/=b means a=a/b\n",
    "\n",
    "rmse2 = np.sqrt(((error)**2 * weights_lat).mean(xr.ALL_DIMS))  \n",
    "rmse2 # 1079.88382283\n",
    "\n",
    "# RMSE without weights : 1269.5848\n",
    "# RMSE with weights : 1079.88382283\n",
    "# weight가 있으면 error가 줄어든다. 왜일까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false"
   },
   "source": [
    "So we get a climatological RMSE of 1080 m^2/s^2 which is very similar to the RMSE we get for the climatology for all training years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false"
   },
   "source": [
    "## Train a neural network\n",
    "\n",
    "Now let's train a simple convolutional neural network. We are using several functions defined in `src/train_nn.py`. You can use and modify these or write your own function for data loading etc. \n",
    "\n",
    "For more information on the the networks, check out `notebooks/3-cnn-example.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_nn import * # import DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This limits TF memory usage on the GPU\n",
    "# limit_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false"
   },
   "source": [
    "First, we need to create the data generators for training, validation and testing. The main reason why we are using data generators instead of just loading the data as Numpy arrays is that this would require loading the same data twice since the features and targets are the same fields, just offset in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32 # batch_size\n",
    "lead_time = 5*24 # the range of time for training data time right before validation data time. the unit is an hour (5 days * 24 hours)\n",
    "var_dict = {'z': None} # the hyperparameter of DataGenerator function: define variable and it's level as dictionary type {'var': level}. \n",
    "# in this case, it means that load variable 'z(z500)' \n",
    "# if data is of single level, Use 'None' for level\n",
    "# 'z' variable has only one level, z500. So, {'z': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataGenerator() : transpose Xarray data to Numpy array\n",
    "# as the input, z500 has 4 coordinates\n",
    "# lat: 32-d, lon: 64-d, time: 350640-d(1979-01-01 T00:00:00 to 2018-12-31 T23:00:00), level: 1-d (z고도500)\n",
    "# as the output, numpy array X: (batch_size, 32, 64, 1), y: (batch_size, 32, 64, 1)\n",
    "\n",
    "ds = z500\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['lat'] # able to call via coordinate name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['z'] # able to call via variable name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.score import *\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv2D, Lambda, LeakyReLU\n",
    "import tensorflow.keras.backend as K\n",
    "from configargparse import ArgParser\n",
    "\n",
    "\n",
    "batch_size = 32 \n",
    "lead_time = 5*24 # ?????\n",
    "var_dict = {'z': None} # {variable name : levels}. in this case, extract the values of any level from the 'z' variable\n",
    "\n",
    "shuffle = True\n",
    "load = False\n",
    "\n",
    "ds = z500\n",
    "data = []\n",
    "generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "#eg. generic_level = xr.DataArray(np.random.randn(2,3,1), dims=['lat', 'lon', 'time'], coords={'lat': [0,1], 'lon' : [0,1,2], 'time' : [1]})\n",
    "\n",
    "\n",
    "for var, levels in var_dict.items():\n",
    "    try:\n",
    "        data.append(ds[var].sel(level=levels)) # dimension 'level'이 존재하면 data에 append\n",
    "    except ValueError:\n",
    "        data.append(ds[var].expand_dims({'level': generic_level}, axis=1)) # dimension 'level'이 존재하지 않으면 axis 추가\n",
    "data ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean = data.mean(('time', 'lat', 'lon')).compute() \n",
    "std = data.std('time').mean(('lat', 'lon')).compute() \n",
    "# Normalize\n",
    "data = (data - mean) / std\n",
    "n_samples = data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "init_time = data.isel(time=slice(None, -lead_time)).time\n",
    "valid_time = data.isel(time=slice(lead_time, None)).time\n",
    "\n",
    "\n",
    "#Updates indexes after each epoch\n",
    "def on_epoch_end():\n",
    "    idxs = np.arange(n_samples)\n",
    "    if shuffle == True:\n",
    "        np.random.shuffle(idxs)\n",
    "        \n",
    "on_epoch_end()\n",
    "\n",
    "\n",
    "# For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "if load: \n",
    "    print('Loading data into RAM'); \n",
    "    data.load()\n",
    "\n",
    "\n",
    "#Denotes the number of batches per epoch\n",
    "int(np.ceil(n_samples / batch_size)) # batch 개수\n",
    "\n",
    "#Generate one batch of data\n",
    "idxs = idxs[i * batch_size:(i + 1) * batch_size]\n",
    "X = data.isel(time=idxs).values\n",
    "y = data.isel(time=idxs + lead_time).values\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataGenerator() : transpose Xarray data to Numpy array\n",
    "\n",
    "# Use 2015 for training and 2016 for validation\n",
    "dg_train = DataGenerator( \n",
    "    z500.sel(time=slice('1979', '2015')), var_dict, lead_time, batch_size=bs, load=True) # load=True : load data into RAM\n",
    "# do we need to shuffle for time series data?\n",
    "# yes. because the time series is not stationary, so contiguous data is likely to be highly correlated.\n",
    "# no. the data is not being randomly shuffled before splitting. This is for two reasons.\n",
    "# chopping the data is still possible after splited windows of consecutive samples.\n",
    "# the validation/test results are more realistic, being evaluated on data collected after the model was trained.\n",
    "dg_valid = DataGenerator(\n",
    "    z500.sel(time=slice('2016', '2016')), var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False)\n",
    "# mean, std are used to normalize data\n",
    "# mean: If None, compute mean from data. 전체 시간, 위치를 통틀어 평균 (so, squeeze 3 axis - lon, lat, time)\n",
    "# std: If None, compute standard deviation from data. 전 지구의 시간별 std (squeeze 2 axis - lon, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now also a generator for testing. Important: Shuffle must be False!\n",
    "dg_test = DataGenerator(z500.sel(time=slice('2017', '2018')), # Limiting the data for Binder, slice=(start=0, end=the end, step=12)\n",
    "                        var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dg_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches have dimensions [batch_size, lat, lon, channels]\n",
    "X.shape, y.shape\n",
    "\n",
    "# 32-D latitude, 64-D longitude, channel=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false"
   },
   "source": [
    "Now let's build a simple fully convolutional network. We are using periodic convolutions in the longitude direction. These are defined in `train_nn.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple fully convolutional network\n",
    "\n",
    "cnn = keras.models.Sequential([\n",
    "    PeriodicConv2D(filters=32, kernel_size=5, conv_kwargs={'activation':'relu'}, input_shape=(32, 64, 1,)), # in the input_shape, except batch dimension\n",
    "    PeriodicConv2D(filters=1, kernel_size=5) # input format information is only needed at the first layer\n",
    "])\n",
    "\n",
    "### the another coding method (equivalent above) ###\n",
    "'''\n",
    "# create model\n",
    "cnn = Sequential()\n",
    "\n",
    "# add model layers\n",
    "cnn.add(PeriodicConv2D(filters=32, kernel_size=5, conv_kwargs={'activation':'relu'}, input_shape=(32, 64, 1,)))\n",
    "cnn.add(PeriodicConv2D(filters=1, kernel_size=5))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For a mean squared error regression problem\n",
    "cnn.compile(keras.optimizers.Adam(learning_rate=1e-4), loss='mse')\n",
    "\n",
    "### the another coding method (equivalent above) ###\n",
    "cnn.compile(optimizer = 'Adam',\n",
    "           loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a little bit ;)\n",
    "cnn.fit(dg_train, epochs=1, validation_data=dg_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false"
   },
   "source": [
    "### Create a prediction and compute score\n",
    "\n",
    "Now that we have a model (albeit a crappy one) we can create a prediction. For this we need to create a forecast for each forecast initialization time in the testing range (2017-2018) and unnormalize it. We then convert the forecasts to a Xarray dataset which allows us to easily compute the RMSE. All of this is taken care of in the `create_predictions()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = create_predictions(cnn, dg_test)\n",
    "# create_predictions(model, test dataset)\n",
    "# create_predictions() : generate the prediction. then, Unnormalize and collect the predicted data into xarray\n",
    "# preds = preds * dg_test.std.values + dg_test.mean.values  # Unnormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_weighted_rmse(preds.z, z500_test).load()\n",
    "# 923.90897686\n",
    "\n",
    "\n",
    "### RMSE without cosine weighting\n",
    "error = preds.z - z500_test\n",
    "\n",
    "rmse3 = np.sqrt(((error)**2).mean()) \n",
    "rmse3 # 1462.5831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = '2017-03-02T00'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "p1= z500_test.sel(time=time).plot(ax=ax1) # ground truth\n",
    "p2= preds.sel(time=time).z.plot(ax=ax2); # prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "\n",
    "time = '2017-03-02T00'\n",
    "p1= z500_test.sel(time=time).plot(subplot_kws=dict(projection=ccrs.Orthographic(130, 35), facecolor=\"gray\"),\n",
    "                                  transform = ccrs.PlateCarree(),\n",
    "                                 ) # ground truth\n",
    "\n",
    "p1.axes.set_global()\n",
    "p1.axes.coastlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = '2017-03-02T00'\n",
    "p2= preds.sel(time=time).z.plot(subplot_kws=dict(projection=ccrs.Orthographic(130, 35), facecolor=\"gray\"),\n",
    "                                  transform = ccrs.PlateCarree(),\n",
    "                                 ); # prediction\n",
    "p2.axes.set_global()\n",
    "p2.axes.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End\n",
    "\n",
    "This is the end of the quickstart guide. Please refer to the Jupyter notebooks in the `notebooks` directory for more examples. If you have questions, feel free to ask them as a Github Issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
